{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931b2fd0-034d-4c56-845d-88d502930833",
   "metadata": {},
   "source": [
    "# Cross-Correltation of Ambient Seismic Noise\n",
    "#### A tutorial set for learning the basics of the cross-correlation method for ambient seismic noise correlation\n",
    "\n",
    "By Thomas Lee, for personal use and final project for EPS522-03: Computational Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fd0bd1-3504-442f-8e89-442ba62751ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b30e6-3799-4c72-acbe-ebf1670f09b0",
   "metadata": {},
   "source": [
    "##### Correlation Basics\n",
    "\n",
    "Correlation is a simple mathematic operation that measures the similarity of two functions. Generally, the correlation can be found by taking two functions and multiplying together the output for each function that corresponds to a given input. Then, the multiplied pairs are all summed to give a single correlation value for the two functions. The mathematical notation is quite simple but difficult to show in a Jupyter notebook.\n",
    "\n",
    "An extremely simple example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2102be8-7793-4687-beee-9f2b876049fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [3, 5, 2]\n",
    "\n",
    "correlation= a[0]*b[0] + a[1]*b[1] + a[2]*b[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad110fe-d17e-47e0-a1b5-cef3e7df673a",
   "metadata": {},
   "source": [
    "So, in this specific case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71aeb60-bafa-4471-93ba-16ffe5b5a5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of functions a and b: 19\n"
     ]
    }
   ],
   "source": [
    "correlation = (1*3) + (2*5) + (3*2)\n",
    "\n",
    "print(f'Correlation of functions a and b: {correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7600d606-dc98-4975-a809-969d35dc6608",
   "metadata": {},
   "source": [
    "We make several important observations with this basic example. \n",
    "\n",
    "One, we don't even need to know the domain as long as we know that the functions have the same lengths. This means that in a sense, our correlation isn't inherently tied to the absolute values of the domain, just the relative step-size in the domain.\n",
    "\n",
    "Two, correlation is inherently a unitless measurement. It is more useful for relative comparisons than an absolute measurement.\n",
    "\n",
    "Lets's take a look at an example setup below to see what this means in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ecabe7-5805-4f15-bab5-d4b4e4dd944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three separate functions, each of the same length\n",
    "a = [1, -2, 4, -8]\n",
    "b = [100, -200, 400, -800]\n",
    "c = [2, -4, 8, -16]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8007e5b7-d9d1-497d-9222-68d7d31fd8af",
   "metadata": {},
   "source": [
    "These three functions are all have the exact same shape, but b and c are both scaled versions of a.\n",
    "If our goal is to find a measurement that tells us the similiarity between two functions, we may run into an issue using the basic correlation.\n",
    "Below I import a correlation function contained in this module for easier reading, but it operationally does the exact same thing as the first example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023db73-48c2-4720-94e4-e487e77c2978",
   "metadata": {},
   "source": [
    "from correlation_basics.basic_correlation import correlation\n",
    "\n",
    "correlation_ab = correlation(a, b)\n",
    "print(correlation_ab)\n",
    "correlation_ac = correlation(a, c)\n",
    "print(correlation_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1ff9f-fd81-4c07-9c1e-7bedaa8c992e",
   "metadata": {},
   "source": [
    "Even though these functions have the exact same shape, a and b have a much higher correlation due to b's large magnitude. One could argue that a and c are in fact MORE similar due to their more similar magnitudes. To solve this, one may use normalized correlation. It turns out to be not useful for ambient noise correlations as we care about maintaining magnitude differences, but normalized uses a weighted magnitude measurement to bring the value of correlation to between -1 and 1. This means it may give a more accurate sense of how similarly shaped two functions are, and can be used to compare all kinds of different functions regardless of their magnitude. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d52a2f1e-aa5e-40b3-9a41-4a0003b1478b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "1.0\n",
      "1.0\n",
      "0.07866339502806552\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from correlation_basics.normalized_correlation import normalized_correlation\n",
    "\n",
    "d = [-5, 5, 10, 2]\n",
    "\n",
    "norm_corr_ab = normalized_correlation(a, b)\n",
    "print(norm_corr_ab)\n",
    "norm_corr_ac = normalized_correlation(a, c)\n",
    "print(norm_corr_ac)\n",
    "norm_corr_ad = normalized_correlation(a, d)\n",
    "print(norm_corr_ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a906231-5c8e-48b7-96e1-cc77204a5b36",
   "metadata": {},
   "source": [
    "As we can see, the correlations for a and b, and a and c, now both return 1 since they have the same shape. A four random function that does not match in shape, d, is normally correlated with a to show the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c02c3-2bf2-4be2-9616-69a4d0fb1fee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross-correlation: here it gets spicy\n",
    "\n",
    "We've seen that correlation is just a unitless measurement of how similar two functions, or signals in the case of ambient seismic noise, are to one another. Cross-correlation asks the question,\"What if we took the second function, offset it to the left, and then swept it across the top function and measured the correlation of the two offset signals at every point?\"\n",
    "\n",
    "Describing this in words is difficult, so I've implemented an interactive ipywidget below to show the basic setup. For two functions with length L, the second function is domain shifted to the left by an amount L. It could be said that the second function is now at an offset of -L. Then, the correlation is measured. Since these two functions have no overlap, and anything outside their data windows is just taken to be 0, the correlation should be 0. \n",
    "\n",
    "Imagine then moving the second function slightly to the right by an amount Delta. Delta is arbitary, but for real data it usually matches the sampling rate of the instrument. The second function now sits at an offset of -L+Delta. The correlation is then measured again, this time ONE data point should overlap between the two functions and a small correlation value will appear. Imagine repeating this process for every Delta-step until the second function is offset by positive L, by which point the two functions do not overlap and the correlation goes back to 0. We would then be left with a correlation value between the two functions for every step L, which we can interpret as a function with domain -L to L and time steps of Delta. This is a cross-correlation function.\n",
    "\n",
    "### Example Interactive Cross-Correlation\n",
    "\n",
    "The interactive plot below shows two functions and their cross-correlation value, and how we sweep the second function across the first and take the correlation at every point to make a cross-correlation function.\n",
    "\n",
    "Notice that despite our functions needing 300 points to show the full sweep, the cross-correlation function only has 200 points, twice the amount of points in the original functions.\n",
    "\n",
    "Play around with the two functions! Adjust their amplitudes, frequencies, phase shifts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95bd0518-ebe3-4358-82ee-9a10f8bec500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a92c1e69964f9e99edd08428c504bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='offset', min=-100), Output()), _dom_classes=('widget-intâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function correlation_basics.teaching_cross_correlation.correlate_interactive(a, b, offset)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import correlation_basics.teaching_cross_correlation as tcc\n",
    "import numpy as np\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "\n",
    "times = np.arange(0, 100, 1)\n",
    "func1 = np.sin(times)\n",
    "func2 = np.cos(times)\n",
    "length = len(func1)\n",
    "\n",
    "#tcc.offset_view(func1, func2, -50)\n",
    "\n",
    "interact(tcc.correlate_interactive, a=fixed(func1),b=fixed(func2),offset=widgets.IntSlider(min=-length, max=length, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8583d39-4e08-45c5-b64b-9c37e346a431",
   "metadata": {},
   "source": [
    "# Applying To Ambient Seismic Noise\n",
    "\n",
    "The basic idea behind ambient noise correlations is that if we assume an equipotential random noise field, meaning noise is completely random with the same amplitude distribution in all directions from our point of measurement, then we can treat the cross-correlation between two seismic stations as an earthquake where one station acts as the receiver and the other acts as a \"virtual source\". Assuming that noise is truly random in all directions, then the only coherence, and therefore the signal that we get from a cross-correlation, should be from noise that travels from one station to the other. This is, as all things in seismology, just a Green's function, which is also exactly what an earthquake is. \n",
    "\n",
    "Over very short time-periods, real cross-correlations will be very noisy. However, say you could take several years worth of data and cut it into hour long windows, find the cross-correlation function between two stations for every single one of those windows, and stack it. Coherent signals will constructively stack and incoherent noise will destructively stack. \n",
    "\n",
    "Usually the signals left after this stacking process are surface waves, primarily Rayleigh waves, traveling from the virtual source station to the receiver station. However, it may also be possible with enough data, small enough windows, and good enough filtering to recover useable body waves (check back in on me in 4 years to see if that was right). From these surface waves we can recover phase and group velocities, which can be used in tomographic inversions because different period surface waves are sensitive to the seismic velocity at different depths.\n",
    "\n",
    "#### Part 1: Downloading Data with ambient_download module\n",
    "\n",
    "ObsPy is the standard seismology package for Python used by pretty much everyone in some capacity and it has some really convenient functions for downloading seismic data from data centers, mostly IRIS. None of them are intended to be used specifically for large amounts of continuous data - it's far more common that people are interested in short time windows surrounding events (mostly earthquakes). As such, it took some experimenting to figure out the best way to get continuous data.\n",
    "\n",
    "Using obspy.clients.fdsn.client.Client.get_waveforms sends an individual request for a given amount of time data. If we need data for N hour-long windows, this method requires sending in N requests. \n",
    "obspy.clients.fdsn.mass_downloader is capable of taking in all of those requests in one single function, but it still sends each request individually.\n",
    "I made functions using both of these methods that download the pre-cut windows, but it turns out that if each request takes T seconds, then N requests will take T * N seconds, which is a long time if a years worth of data for one single station is cut into 8760 windows. Imagine that for a couple hundred stations.\n",
    "\n",
    "However, ObsPy's built in cutting capabilities are very fast. The better method is to use a single request to download a longer window of time, say a year, and cut it locally. For N windows and a download time of T2 seconds (T2 > T from the first method, but only marginally), the overall calculation time is more like T2 + N * very_small_number. This method is way faster.\n",
    "\n",
    "Download_Main shows how this is intended to be used, and another example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d04e2da-f29d-46a2-b881-a957baa2d5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: string given for time_window, now assuming given string is the end time\n",
      "window_remainder 0.0\n",
      "Cutting 86400.0 seconds of data into 24 windows\n",
      "Successfully calculated cut windows, cutting now\n",
      "Successfully saved traces\n",
      "Warning: string given for time_window, now assuming given string is the end time\n",
      "window_remainder 0.0\n",
      "Cutting 86400.0 seconds of data into 24 windows\n",
      "Successfully calculated cut windows, cutting now\n",
      "Successfully saved traces\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0 Trace(s) in Stream:\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ambient_download.download_trace import download_trace\n",
    "from ambient_download.cut_data import cut_traces_into_windows\n",
    "from ambient_download.save_stream import save_stream\n",
    "import obspy\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "# Defining the overall time range we want to correlate. For speed we'll just use a day in this example.\n",
    "starttime = \"2022-01-01T00:00:00.000\" # Start time in the format used by UTCDateTime, which matches the IRIS format\n",
    "endtime = \"2022-01-02T00:00:00.000\" # End time\n",
    "    \n",
    "stream = download_trace(client=\"IRIS\", # Downloading the trace\n",
    "                        network=\"IU\",\n",
    "                        station=\"TUC\",\n",
    "                        location=\"00\",\n",
    "                        channel=\"BHZ\",\n",
    "                        starttime=starttime,\n",
    "                        timewindow=endtime)\n",
    "    \n",
    "cut_stream = cut_traces_into_windows(stream,windowlength=3600) # Window length in seconds, set to 3600 for an hour\n",
    "    \n",
    "sort_method_list = ['station','year','julday'] # Parameters that should be used for the file tree containing the files\n",
    "    \n",
    "save_stream(stream=cut_stream,main_folder='./Example_Traces', # Saving the files, in a file tree designated by sort_method_list\n",
    "                sort_method=sort_method_list, adding_data=True)\n",
    "    \n",
    "print('Successfully saved traces')\n",
    "cut_stream.clear() # Clear the stream to save on memory or to avoid breaking things if you're doing multiple downloads\n",
    "\n",
    "# Repeating the process for ANMO\n",
    "\n",
    "stream = download_trace(client=\"IRIS\", # Downloading the trace\n",
    "                        network=\"IU\",\n",
    "                        station=\"ANMO\",\n",
    "                        location=\"00\",\n",
    "                        channel=\"BHZ\",\n",
    "                        starttime=starttime,\n",
    "                        timewindow=endtime)\n",
    "    \n",
    "cut_stream = cut_traces_into_windows(stream,windowlength=3600) # Window length in seconds, set to 3600 for an hour\n",
    "    \n",
    "sort_method_list = ['station','year','julday'] # Parameters that should be used for the file tree containing the files\n",
    "    \n",
    "save_stream(stream=cut_stream,main_folder='./Example_Traces', # Saving the files, in a file tree designated by sort_method_list\n",
    "                sort_method=sort_method_list, adding_data=True)\n",
    "    \n",
    "print('Successfully saved traces')\n",
    "cut_stream.clear() # Clear the stream to save on memory or to avoid breaking things if you're doing multiple downloads\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9030cc-d7ca-4979-84d1-65316de18b2f",
   "metadata": {},
   "source": [
    "#### Part 2: Correlating The Data\n",
    "\n",
    "- Find corresponding files\n",
    "- Filter them using the method ascribed in Jiang et. al,. 2023\n",
    "- Correlate them\n",
    "- Stack the correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32422428-56a4-4f0b-a12f-6b05c28d7977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m pairs_list \u001b[38;5;241m=\u001b[39m prep\u001b[38;5;241m.\u001b[39mcreate_corresponding_files_list(df1, df2) \u001b[38;5;66;03m# Create a list of file pairs\u001b[39;00m\n\u001b[0;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m---> 13\u001b[0m xcorr_list, xcorr_times \u001b[38;5;241m=\u001b[39m corr\u001b[38;5;241m.\u001b[39mmulti_correlate(pairs_list,time_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseconds\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Cross-correlating them\u001b[39;00m\n\u001b[0;32m     14\u001b[0m xcorr_stack \u001b[38;5;241m=\u001b[39m corr\u001b[38;5;241m.\u001b[39mxcorr_stack(xcorr_list)\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(xcorr_times,xcorr_stack)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Cross_Correlation\\correlation\\cross_correlate_ambient_noise.py:163\u001b[0m, in \u001b[0;36mmulti_correlate\u001b[1;34m(pair_list, time_method)\u001b[0m\n\u001b[0;32m    160\u001b[0m xcorr_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m pair_list:\n\u001b[1;32m--> 163\u001b[0m     xcorr, xcorr_times, meta \u001b[38;5;241m=\u001b[39m cross_correlate_ambient_noise(pair,time_method)\n\u001b[0;32m    164\u001b[0m     xcorr_list\u001b[38;5;241m.\u001b[39mappend(xcorr)\n\u001b[0;32m    166\u001b[0m xcorr_list_fixed \u001b[38;5;241m=\u001b[39m check_correlation_length(xcorr_list)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\Cross_Correlation\\correlation\\cross_correlate_ambient_noise.py:90\u001b[0m, in \u001b[0;36mcross_correlate_ambient_noise\u001b[1;34m(pair, time_method)\u001b[0m\n\u001b[0;32m     86\u001b[0m delta_new \u001b[38;5;241m=\u001b[39m stream_filtered[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     88\u001b[0m stream\u001b[38;5;241m.\u001b[39mdetrend() \u001b[38;5;66;03m# Remove trend again, after Jiang et. al., 2023\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m xcorr \u001b[38;5;241m=\u001b[39m correlate(stream_filtered[\u001b[38;5;241m0\u001b[39m], stream_filtered[\u001b[38;5;241m1\u001b[39m],normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,shift\u001b[38;5;241m=\u001b[39mnpts)\n\u001b[0;32m     92\u001b[0m xcorr_times \u001b[38;5;241m=\u001b[39m create_ambient_times(npts,delta_new,time_method)\n\u001b[0;32m     94\u001b[0m meta \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\obspy\\signal\\cross_correlation.py:159\u001b[0m, in \u001b[0;36mcorrelate\u001b[1;34m(a, b, shift, demean, normalize, method)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# choose the usually faster xcorr function for each method\u001b[39;00m\n\u001b[0;32m    158\u001b[0m _xcorr \u001b[38;5;241m=\u001b[39m _xcorr_padzeros \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m _xcorr_slice\n\u001b[1;32m--> 159\u001b[0m cc \u001b[38;5;241m=\u001b[39m _xcorr(a, b, shift, method)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaive\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    161\u001b[0m     norm \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39msum(a \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(b \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\obspy\\signal\\cross_correlation.py:62\u001b[0m, in \u001b[0;36m_xcorr_slice\u001b[1;34m(a, b, shift, method)\u001b[0m\n\u001b[0;32m     59\u001b[0m     shift \u001b[38;5;241m=\u001b[39m mid\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shift \u001b[38;5;241m>\u001b[39m mid:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Such a large shift is not possible without zero padding\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _xcorr_padzeros(a, b, shift, method)\n\u001b[0;32m     63\u001b[0m cc \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mcorrelate(a, b, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m, method\u001b[38;5;241m=\u001b[39mmethod)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cc[mid \u001b[38;5;241m-\u001b[39m shift:mid \u001b[38;5;241m+\u001b[39m shift \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(cc) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\obspy\\signal\\cross_correlation.py:49\u001b[0m, in \u001b[0;36m_xcorr_padzeros\u001b[1;34m(a, b, shift, method)\u001b[0m\n\u001b[0;32m     47\u001b[0m     b \u001b[38;5;241m=\u001b[39m _pad_zeros(b, dif \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     a \u001b[38;5;241m=\u001b[39m _pad_zeros(a, \u001b[38;5;241m-\u001b[39mdif \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mcorrelate(a, b, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, method\u001b[38;5;241m=\u001b[39mmethod)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\obspy\\signal\\cross_correlation.py:35\u001b[0m, in \u001b[0;36m_pad_zeros\u001b[1;34m(a, num, num2)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     num2 \u001b[38;5;241m=\u001b[39m num\n\u001b[1;32m---> 35\u001b[0m hstack \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mzeros(num, dtype\u001b[38;5;241m=\u001b[39ma\u001b[38;5;241m.\u001b[39mdtype), a, np\u001b[38;5;241m.\u001b[39mzeros(num2, dtype\u001b[38;5;241m=\u001b[39ma\u001b[38;5;241m.\u001b[39mdtype)]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mhstack(hstack)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from correlation import prep_files as prep\n",
    "from correlation import cross_correlate_ambient_noise as corr\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "station1_files, station2_files = prep.find_station_files('ANMO', 'TUC', './Example_Traces') # Finding correlation files\n",
    "df1, df2 = prep.get_info_from_file_name(station1_files, station2_files, name_structure=None) # Pulling them into a dataframe using metadata\n",
    "                                                                                             # from the file name\n",
    "pairs_list = prep.create_corresponding_files_list(df1, df2) # Create a list of file pairs\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "xcorr_list, xcorr_times = corr.multi_correlate(pairs_list,time_method='seconds') # Cross-correlating them\n",
    "xcorr_stack = corr.xcorr_stack(xcorr_list)\n",
    "plt.plot(xcorr_times,xcorr_stack)\n",
    "plt.title('Stacked TUC-ANMO Cross-Correlation, No Running Average Filter')\n",
    "plt.show()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "print(f'Correlation took {end_time - start_time} seconds')\n",
    "\n",
    "xcorr_averaged, xcorr_averaged_times = corr.running_average_filter(xcorr_stack,xcorr_times,avg_window=20)\n",
    "plt.plot(xcorr_averaged_times, xcorr_averaged,'r-')\n",
    "plt.title('Stacked TUC-ANMO Cross-Correlation, 20 Second Running Average')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447aef9-bd7d-4f3f-8de9-41694f1810cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
